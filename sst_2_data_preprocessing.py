# -*- coding: utf-8 -*-
"""SST-2_data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hcJZYcfIord6atkZq8fzlYZSW7aB5DE-
"""

!pip install datasets

import torch
from datasets import load_dataset
raw_datasets = load_dataset("glue", "sst2")

raw_datasets

pd = raw_datasets['train'].to_pandas()
pd

# 2. Class Distribution
labels = raw_datasets["train"]["label"]
label_counts = torch.bincount(torch.tensor(labels))
print("Class Distribution:")
print(label_counts)

# 3. Text Length
text_lengths = [len(text.split()) for text in raw_datasets["train"]["sentence"]]
print("Max text length:", max(text_lengths))
print("Min text length:", min(text_lengths))
print("Average text length:", sum(text_lengths) / len(text_lengths))

# Importing necessary libraries
import matplotlib.pyplot as plt

# Get labels
labels = raw_datasets["train"]["label"]

# Count occurrences of each label
label_counts = torch.bincount(torch.tensor(labels))

# Plotting the distribution
plt.bar(range(len(label_counts)), label_counts.numpy())
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Class Distribution')
plt.show()

# 4. Word2Vec Analysis
from gensim.models import Word2Vec

sentences = raw_datasets["train"]["sentence"]

# Tokenize the sentences
tokenized_sentences = [sentence.split() for sentence in sentences]
# Train Word2Vec model
word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)


# Example: Get most similar words to a given word
print("Most similar words to 'good':", word2vec_model.wv.most_similar("good"))

# Plotting word frequency distribution
word_frequencies = sorted(word2vec_model.wv.key_to_index.items(), key=lambda x: x[1], reverse=True)
words, frequencies = zip(*word_frequencies[:50])  # Choose top 50 words by frequency
plt.bar(words, frequencies)
plt.xticks(rotation=90)
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.title('Word Frequency Distribution')
plt.show()

# 5. Word Embedding Visualization
# Visualizing Word2Vec embeddings using t-SNE
from sklearn.manifold import TSNE
import numpy as np
# Get Word2Vec embeddings
word_embeddings = np.array([word2vec_model.wv[word] for word in word2vec_model.wv.key_to_index.keys()])

# Reduce dimensionality using t-SNE
tsne = TSNE(n_components=2, random_state=42)
word_embeddings_tsne = tsne.fit_transform(word_embeddings)

# Enable interactive mode for zooming
plt.figure(figsize=(30,30))
plt.scatter(word_embeddings_tsne[:, 0], word_embeddings_tsne[:, 1], alpha=0.5)

# Add annotations for each data point (word)
for i, word in enumerate(word2vec_model.wv.key_to_index.keys()):
    plt.annotate(word, (word_embeddings_tsne[i, 0], word_embeddings_tsne[i, 1]), alpha=0.7)

plt.title('Word Embeddings Visualization with Annotations')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.gca().set_aspect('auto', adjustable='box')  # Set aspect ratio to 'auto' for interactive zooming
plt.show()

# 4. Tokenization
from transformers import AutoTokenizer, DataCollatorWithPadding
checkpoint = "bert-base-uncased" #we will use bert base uncased for fine tune
tokenizer = AutoTokenizer.from_pretrained(checkpoint) #get the tokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
example_text = raw_datasets["train"]["sentence"][0]
tokens = tokenizer.tokenize(example_text)
print("Example Text:", example_text)
print("Tokenized Text:", tokens)